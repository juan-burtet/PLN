{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gQivtoERTspU"
   },
   "source": [
    "# Baixando Dados do NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XfRgixOmTnBk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package brown to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /home/juan-burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/juan-burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package floresta to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    | Downloading package lin_thesaurus to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    | Downloading package moses_sample to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    | Downloading package nps_chat to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
      "[nltk_data]    | Downloading package opinion_lexicon to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package paradigms to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pil to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package ppattach to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package propbank to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    | Downloading package ptb to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package pros_cons to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package qc to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    | Downloading package rte to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package semcor to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package state_union to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package timit to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /home/juan-burtet/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | Downloading package rslp to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/juan-burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package spanish_grammars to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package large_grammars to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package tagsets to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    | Downloading package mte_teip5 to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/juan-burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /home/juan-burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /home/juan-burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package vader_lexicon to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    | Downloading package porter_test to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to /home/juan-\n",
      "[nltk_data]    |     burtet/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CSs8hgOjTiWf"
   },
   "source": [
    "# Baixando Modelos do spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NYxu_HtCS0Hs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('pt_core_news_sm')\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('es_core_news_sm')\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "# !python3 -m spacy download en_core_web_sm\n",
    "# !python3 -m spacy download pt_core_news_sm\n",
    "# !python3 -m spacy download es_core_news_sm\n",
    "# !python3 -m spacy download de_core_news_sm\n",
    "#!python -m spacy validate\n",
    "\n",
    "import spacy.cli\n",
    "\n",
    "modelos = [\"en_core_web_sm\", \"pt_core_news_sm\", \"es_core_news_sm\", \"de_core_news_sm\"]\n",
    "for model in modelos:\n",
    "  spacy.cli.download(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ApaBADKFWkz2"
   },
   "source": [
    "# Lematização\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rwyPu_lKKGvP"
   },
   "source": [
    "## Lematização usando NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 90471,
     "status": "ok",
     "timestamp": 1567606202728,
     "user": {
      "displayName": "Larissa Freitas",
      "photoUrl": "",
      "userId": "14992942519253748435"
     },
     "user_tz": 180
    },
    "id": "o46sPzImRF15",
    "outputId": "7b09d857-3075-41c2-bd08-1e30cf290a52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bat\n",
      "are\n",
      "foot\n",
      "rock\n",
      "corpus\n",
      "strip\n",
      "stripe\n",
      "better : good\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"bats\"))\n",
    "print(lemmatizer.lemmatize(\"are\"))\n",
    "print(lemmatizer.lemmatize(\"feet\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\")) \n",
    "print(lemmatizer.lemmatize(\"corpora\")) \n",
    "\n",
    "print(lemmatizer.lemmatize(\"stripes\", 'v'))  \n",
    "print(lemmatizer.lemmatize(\"stripes\", 'n'))  \n",
    "\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9pbk0cG2Qh7P"
   },
   "source": [
    "## Lematização usando spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_tfmMXzEQkAG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apples apple\n",
      "and and\n",
      "oranges orange\n",
      "are be\n",
      "similar similar\n",
      ". .\n",
      "Boots boot\n",
      "and and\n",
      "hippos hippos\n",
      "are be\n",
      "n't not\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(u\"Apples and oranges are similar. Boots and hippos aren't.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, token.lemma_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vDliqEmgnL8A"
   },
   "source": [
    "## Comparação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kFeNhZDanDKC"
   },
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 90712,
     "status": "ok",
     "timestamp": 1567606202994,
     "user": {
      "displayName": "Larissa Freitas",
      "photoUrl": "",
      "userId": "14992942519253748435"
     },
     "user_tz": 180
    },
    "id": "kbSS40_ojqRN",
    "outputId": "b30cc4a1-e97d-470a-ad04-6ab3d2bddf8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Article: Lemmatisation (or lemmatization) in linguistics is \n",
      "the process of grouping together the inflected forms of a word so \n",
      "they can be analysed as a single item, identified by the word's lemma, \n",
      "or dictionary form.\n",
      "\n",
      "Original : forms, New: form\n",
      "Original : as, New: a\n"
     ]
    }
   ],
   "source": [
    "#import library\n",
    "\n",
    "import nltk \n",
    "#print('NLTK Version: %s' % (nltk.__version__))\n",
    "wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "#check\n",
    "\n",
    "article = \"\"\"Lemmatisation (or lemmatization) in linguistics is \n",
    "the process of grouping together the inflected forms of a word so \n",
    "they can be analysed as a single item, identified by the word's lemma, \n",
    "or dictionary form.\"\"\"\n",
    "\n",
    "tokens = nltk.word_tokenize(article)\n",
    "print('Original Article: %s' % (article))\n",
    "print()\n",
    "for token in tokens:\n",
    "    lemmatized_token = wordnet_lemmatizer.lemmatize(token)\n",
    "    \n",
    "    if token != lemmatized_token:\n",
    "        print('Original : %s, New: %s' % (token, lemmatized_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rt1rXAfFm46F"
   },
   "source": [
    "### spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 90722,
     "status": "ok",
     "timestamp": 1567606202990,
     "user": {
      "displayName": "Larissa Freitas",
      "photoUrl": "",
      "userId": "14992942519253748435"
     },
     "user_tz": 180
    },
    "id": "-FdGZAGCiGK4",
    "outputId": "862e82dd-39ad-4170-9012-f30efad3d877"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Article: Lemmatisation (or lemmatization) in linguistics is \n",
      "the process of grouping together the inflected forms of a word so \n",
      "they can be analysed as a single item, identified by the word's lemma, \n",
      "or dictionary form.\n",
      "\n",
      "Original : Lemmatisation, New: lemmatisation\n",
      "Original : is, New: be\n",
      "Original : grouping, New: group\n",
      "Original : forms, New: form\n",
      "Original : they, New: -PRON-\n",
      "Original : analysed, New: analyse\n",
      "Original : identified, New: identify\n"
     ]
    }
   ],
   "source": [
    "#import library\n",
    "\n",
    "import spacy\n",
    "#print('spaCy Version: %s' % (spacy.__version__))\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#normalize\n",
    "\n",
    "article = \"\"\"Lemmatisation (or lemmatization) in linguistics is \n",
    "the process of grouping together the inflected forms of a word so \n",
    "they can be analysed as a single item, identified by the word's lemma, \n",
    "or dictionary form.\"\"\"\n",
    "\n",
    "doc = spacy_nlp(article)\n",
    "tokens = [token.text for token in doc]\n",
    "print('Original Article: %s' % (article))\n",
    "print()\n",
    "for token in doc:\n",
    "    if token.text != token.lemma_:\n",
    "        print('Original : %s, New: %s' % (token.text, token.lemma_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ErCEPfR9WXQH"
   },
   "source": [
    "# Stemização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "shllUMTmIU_T"
   },
   "source": [
    "### German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 646,
     "status": "ok",
     "timestamp": 1567606246747,
     "user": {
      "displayName": "Larissa Freitas",
      "photoUrl": "",
      "userId": "14992942519253748435"
     },
     "user_tz": 180
    },
    "id": "MKpkM6GqIUpe",
    "outputId": "17238f74-9e4b-473b-d751-9232524772b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'autobahn'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from nltk.stem import SnowballStemmer\n",
    "\n",
    "# stemmer = SnowballStemmer(\"german\") \n",
    "# stemmer.stem(\"Autobahnen\") \n",
    "\n",
    "from nltk.stem.snowball import GermanStemmer\n",
    "\n",
    "stemmer = GermanStemmer()\n",
    "stemmer.stem(\"Autobahnen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0870EsC5tj7u"
   },
   "source": [
    "### Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 643,
     "status": "ok",
     "timestamp": 1567606249278,
     "user": {
      "displayName": "Larissa Freitas",
      "photoUrl": "",
      "userId": "14992942519253748435"
     },
     "user_tz": 180
    },
    "id": "ujBc3KF-tMk3",
    "outputId": "259002ec-45e7-40e0-c72c-a85609fa6fae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'carreter'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SpanishStemmer\n",
    "\n",
    "stemmer = SpanishStemmer()\n",
    "stemmer.stem(\"Carreteras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xet5C_mdtxk6"
   },
   "source": [
    "### Portuguese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1567606250569,
     "user": {
      "displayName": "Larissa Freitas",
      "photoUrl": "",
      "userId": "14992942519253748435"
     },
     "user_tz": 180
    },
    "id": "SL-rzKH-tzYN",
    "outputId": "addef045-6773-4143-cb6b-25b31fa46689"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rodov'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "stemmer = RSLPStemmer()\n",
    "stemmer.stem(\"Rodovias\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "9pbk0cG2Qh7P",
    "rwyPu_lKKGvP",
    "vDliqEmgnL8A",
    "rt1rXAfFm46F",
    "kFeNhZDanDKC",
    "shllUMTmIU_T",
    "0870EsC5tj7u",
    "Xet5C_mdtxk6"
   ],
   "name": "Aula_270819.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
