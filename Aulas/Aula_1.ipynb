{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z7sM8QN5pWoj"
   },
   "source": [
    "# Tokenização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eBDScizvpaam"
   },
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fXvhuznUpU7k"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/juan-\n",
      "[nltk_data]     burtet/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s8J8posTpl-P"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$',\n",
       " '3.88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " '.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " '...',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " '.',\n",
       " 'Thanks',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "s = '''Good muffins cost $3.88\\nin New York.  Please buy me ... two of them.\\n\\nThanks.'''\n",
    "word_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3V8Q9tURpmpE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$',\n",
       " '3',\n",
       " '.',\n",
       " '88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " '.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " '...',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " '.',\n",
       " 'Thanks',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "s = '''Good muffins cost $3.88\\nin New York.  Please buy me ... two of them.\\n\\nThanks.'''\n",
    "wordpunct_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IETxWRCHps2A"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.'],\n",
       " ['Please', 'buy', 'me', '...', 'two', 'of', 'them', '.'],\n",
       " ['Thanks', '.']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "s = '''Good muffins cost $3.88\\nin New York.  Please buy me ... two of them.\\n\\nThanks.'''\n",
    "\n",
    "#sent_tokenize(s)\n",
    "\n",
    "[word_tokenize(t) for t in sent_tokenize(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-p07Bcippuvt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$3.88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " '.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " 'Thanks',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them \\n\\nThanks.\"\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r_2-Bh0kpwm2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cooool',\n",
       " '#dummysmiley',\n",
       " ':',\n",
       " ':-)',\n",
       " ':-P',\n",
       " '<3',\n",
       " 'and',\n",
       " 'some',\n",
       " 'arrows',\n",
       " '<',\n",
       " '>',\n",
       " '->',\n",
       " '<--']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "s0 = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "\n",
    "tknzr.tokenize(s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZWw3Odpepy6X"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In', 'a_little', 'or', 'a_little_bit', 'or', 'a_lot', 'in_spite_of']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "tokenizer = MWETokenizer([('a', 'little'), ('a', 'little', 'bit'), ('a', 'lot')])\n",
    "tokenizer.add_mwe(('in', 'spite', 'of'))\n",
    "tokenizer.tokenize('In a little or a little bit or a lot in spite of'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6VpzPSIGpcjj"
   },
   "source": [
    "## spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 892,
     "status": "ok",
     "timestamp": 1566500978322,
     "user": {
      "displayName": "Larissa Freitas",
      "photoUrl": "",
      "userId": "14992942519253748435"
     },
     "user_tz": 180
    },
    "id": "R7PpeTpetuCQ",
    "outputId": "9cc2b2f3-c4e1-4a77-99d1-67388d00d834"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liebe\n",
      "Grüße\n",
      "!\n",
      "-----------------------------------------------------------\n",
      "¿\n",
      "Cómo\n",
      "estás\n",
      "?\n",
      "-----------------------------------------------------------\n",
      "Oi\n",
      "tudo\n",
      "bem\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "#Alemao\n",
    "\n",
    "from spacy.lang.de import German\n",
    "nlp = German()\n",
    "doc = nlp(\"Liebe Grüße!\")\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "print('-----------------------------------------------------------')  \n",
    "\n",
    "#Espanhol\n",
    "\n",
    "from spacy.lang.es import Spanish\n",
    "nlp = Spanish()\n",
    "doc = nlp(\"¿Cómo estás?\")\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "print('-----------------------------------------------------------')  \n",
    "\n",
    "#Portugues\n",
    "\n",
    "from spacy.lang.pt import Portuguese\n",
    "nlp = Portuguese()\n",
    "doc = nlp(\"Oi tudo bem?\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G2ktEB7ctuiq"
   },
   "source": [
    "### BeautifulSoup, urlopen e re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2925,
     "status": "ok",
     "timestamp": 1567019604159,
     "user": {
      "displayName": "Larissa Freitas",
      "photoUrl": "",
      "userId": "14992942519253748435"
     },
     "user_tz": 180
    },
    "id": "OaDu0KW2pfSH",
    "outputId": "af7ceeb4-5468-4c0f-cf1e-3f58b2df1323"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6.8'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "html = urlopen('https://www.imdb.com/title/tt7343762/?pf_rd_m=A2FGELUUNOQJNL&pf_rd_p=ea4e08e1-c8a3-47b5-ac3a-75026647c16e&pf_rd_r=2CV9738DXC92SV06SQ9B&pf_rd_s=center-1&pf_rd_t=15506&pf_rd_i=moviemeter&ref_=chtmvm_tt_2')\n",
    "\n",
    "soup = BeautifulSoup(html.read())\n",
    "\n",
    "soup.find('span', attrs={'itemprop': 'ratingValue'}).getText()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Aula_210819.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
