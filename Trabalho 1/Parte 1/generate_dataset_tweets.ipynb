{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pt_core_news_sm==2.2.0 from https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.2.0/pt_core_news_sm-2.2.0.tar.gz#egg=pt_core_news_sm==2.2.0 in /home/juan-burtet/.anaconda3/lib/python3.7/site-packages (2.2.0)\n",
      "Requirement already satisfied: spacy>=2.2.0 in /home/juan-burtet/.anaconda3/lib/python3.7/site-packages (from pt_core_news_sm==2.2.0) (2.2.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/juan-burtet/.anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->pt_core_news_sm==2.2.0) (2.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/juan-burtet/.anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->pt_core_news_sm==2.2.0) (2.21.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /home/juan-burtet/.anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->pt_core_news_sm==2.2.0) (0.1.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/juan-burtet/.local/lib/python3.7/site-packages (from spacy>=2.2.0->pt_core_news_sm==2.2.0) (1.16.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/juan-burtet/.anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->pt_core_news_sm==2.2.0) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/juan-burtet/.anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->pt_core_news_sm==2.2.0) (3.0.2)\n",
      "Requirement already satisfied: thinc<7.2.0,>=7.1.1 in /home/juan-burtet/.anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->pt_core_news_sm==2.2.0) (7.1.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/juan-burtet/.anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->pt_core_news_sm==2.2.0) (0.4.1)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /home/juan-burtet/.anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->pt_core_news_sm==2.2.0) (0.9.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /home/juan-burtet/.anaconda3/lib/python3.7/site-packages (from spacy>=2.2.0->pt_core_news_sm==2.2.0) (0.2.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/juan-burtet/.anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->pt_core_news_sm==2.2.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/juan-burtet/.anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->pt_core_news_sm==2.2.0) (1.24.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/juan-burtet/.anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->pt_core_news_sm==2.2.0) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/juan-burtet/.anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->pt_core_news_sm==2.2.0) (2019.3.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /home/juan-burtet/.local/lib/python3.7/site-packages (from thinc<7.2.0,>=7.1.1->spacy>=2.2.0->pt_core_news_sm==2.2.0) (4.36.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('pt_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "!python -m spacy download pt_core_news_sm\n",
    "import pt_core_news_sm\n",
    "from tqdm import tqdm\n",
    "\n",
    "sp = pt_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3157/3157 [00:37<00:00, 84.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset criado!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "\n",
    "def create_dataset():\n",
    "    # Abre o arquivo de leis, para formata-lo igual o CRFtoNER\n",
    "    with open('Datasets/tweets_by_id.txt', 'r') as f_tweets, \\\n",
    "         open('Datasets/ptbr_tweets.csv', 'w+') as file:\n",
    "        \n",
    "        # Abre o arquivo dos NET Tweets em ordem decrescente\n",
    "        df = pd.read_csv('Datasets/dataset.ptbr_tweets.txt', sep=\"\\t\")\n",
    "        df = df.sort_values('tweet_id')\n",
    "        \n",
    "        # Abre todas as linhas do arquivo\n",
    "        tweets = f_tweets.read().split(\"|TWEET_ID|:\")\n",
    "        \n",
    "        # Escreve o header do csv\n",
    "        file.write(\"Sentence #\\tWord\\tPOS\\tTag\\n\")\n",
    "\n",
    "        list_sent = []\n",
    "\n",
    "        # Percorre linha por linha\n",
    "        sent_i = 1\n",
    "        for i, tweet in enumerate(tqdm(tweets)):\n",
    "            # Pula o cabeçalho\n",
    "            if i == 0:\n",
    "                continue\n",
    "            \n",
    "            # Pega o ID e a Mensagem\n",
    "            tweet_id, original = tweet.split(\"|MENSAGEM|: \")\n",
    "            \n",
    "            if \"|SEM INFORMAÇÃO|\" in original:\n",
    "                continue\n",
    "            \n",
    "            # Modificando a string\n",
    "            msg = original.replace(\"'\", \" \")\n",
    "            msg = msg.replace('\"', ' ')\n",
    "            msg = msg.replace(\"“\", ' ')\n",
    "            msg = msg.replace(\"”\", ' ')\n",
    "            msg = msg.replace(\",\", \" , \")\n",
    "            msg = msg.replace(\";\", \" ; \")\n",
    "            msg = msg.replace(\"-\", \" - \")\n",
    "            msg = msg.replace(\"\\n\", \" \")\n",
    "            msg = msg.replace(\"\\t\", \" \")\n",
    "            \n",
    "            # Passa pelo Spacy a mensagem\n",
    "            msg_sp = sp(u\"%s\" % msg)\n",
    "            #msg_sp = sp(msg\n",
    "            \n",
    "            # Retorna um df apenas com os tweets com essa id\n",
    "            aux_df = df.loc[df['tweet_id'] == int(tweet_id)]\n",
    "            \n",
    "            # Faz uma lista com a\n",
    "            # (PALAVRA, POSTAG, TAG)\n",
    "            list_sent = []\n",
    "            for s in msg_sp:\n",
    "                list_sent.append([\n",
    "                    s.text,\n",
    "                    s.pos_,\n",
    "                    'O'\n",
    "                ])\n",
    "                \n",
    "            # Conjunto de palavras\n",
    "            words = [s.text for s in msg_sp]\n",
    "            \n",
    "            # Percorre todas as linhas do DF com o mesmo ID\n",
    "            for index, row in aux_df.iterrows():\n",
    "                \n",
    "                # Se for diferente de \"No entities found in this tweet\",\n",
    "                # Significa que Possui uma entidade neste tweet\n",
    "                if row['named_entity_type'] != 'No entities found in this tweet':\n",
    "                    \n",
    "                    # Tag da entidade\n",
    "                    tag = row['named_entity_type']\n",
    "                    \n",
    "                    # Posição da entidade no texto\n",
    "                    begin = row['start_pos'] -1\n",
    "                    end = row['end_pos'] -1\n",
    "                    \n",
    "                    # Devido alguns erros,\n",
    "                    # a palavra pode estar nessas 3 \n",
    "                    # posições\n",
    "                    palavras = [\n",
    "                        original[begin:end], \n",
    "                        original[(begin+1):(end+1)],\n",
    "                        original[(begin+2):(end+2)],\n",
    "                    ]    \n",
    "                    \n",
    "                    # Percorre todas as palavras\n",
    "                    found = False\n",
    "                    for palavra in palavras:\n",
    "                        \n",
    "                        # Percorre todas as words da sentença\n",
    "                        for count, word in enumerate(words):\n",
    "                            \n",
    "                            # Se deu match, atualiza o Tag\n",
    "                            if palavra == word:\n",
    "                                list_sent[count][2] = tag\n",
    "                                found = True\n",
    "                                break\n",
    "                                \n",
    "                        # Se encontrou, sai do laço\n",
    "                        if found:\n",
    "                            break\n",
    "            \n",
    "            for sent in list_sent:\n",
    "                if sent[0] != ' ':\n",
    "                    file.write(\"Sentence %d\\t\" % sent_i)\n",
    "                    file.write(\"%s\\t\" % sent[0])\n",
    "                    file.write(\"%s\\t\" % sent[1])\n",
    "                    file.write(\"%s\\n\" % sent[2])    \n",
    "            \n",
    "            sent_i += 1\n",
    "            \n",
    "            \n",
    "if not os.path.isfile('Datasets/ptbr_tweets.csv'):\n",
    "    print(\"Criando dataset...\")\n",
    "    create_dataset()\n",
    "\n",
    "create_dataset()\n",
    "print(\"Dataset criado!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
